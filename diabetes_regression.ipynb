{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import random\n",
    "import xgboost as xgb\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "np.set_printoptions(suppress=True) # Suppress scientific notation where possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, precision_recall_curve, accuracy_score, f1_score, fbeta_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import Counter\n",
    "\n",
    "df = pd.read_csv('./diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `readmitted` column currently contains three categorical values for whether a patient is readmitted. We'll simplify the values in the column and designate 0 to mean not readmitted and 1 to signify readmitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['readmitted'].replace({'NO':0, '>30':1, '<30':1}, inplace=True)\n",
    "df['readmitted'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at a heatmap to get an idea of which features are more closely correlated in order to choose predictor variables for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[['age', 'admission_type_id', 'discharge_disposition_id', 'time_in_hospital', 'num_lab_procedures', 'num_procedures', 'num_medications', 'number_diagnoses', 'readmitted']]\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "sns.heatmap(df1.corr(), cmap='coolwarm', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Pre-Processing\n",
    "\n",
    "We'll pre-process data using the following steps:\n",
    "1. Split the data into train, test, and validation sets.\n",
    "2. One-hot encode categorical variables for each set.\n",
    "3. Standardize continous variables for each set.\n",
    "4. Combine the processed features into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into 3: 60% train, 20% validation, 20% test\n",
    "X_train, X_test, label_train, label_test = train_test_split(df[['race', 'gender', 'age', 'time_in_hospital',\n",
    "                                                                'num_lab_procedures', 'num_procedures', 'num_medications',\n",
    "                                                                'diag_1', 'diag_2', 'diag_3']], df['readmitted'],\n",
    "                                                            test_size=0.2, random_state=2018)\n",
    "X_train, X_val, label_train, label_val = train_test_split(df[['race', 'gender', 'age', 'time_in_hospital',\n",
    "                                                              'num_lab_procedures', 'num_procedures', 'num_medications',\n",
    "                                                              'diag_1', 'diag_2', 'diag_3']], df['readmitted'],\n",
    "                                                          test_size=0.25, random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select categorical variables from training set\n",
    "cat_variable = ['race', 'gender', 'diag_1', 'diag_2', 'diag_3']\n",
    "\n",
    "X_train_cat = X_train[cat_variable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate OneHotEncoder object\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False, drop='first')\n",
    "ohe.fit(X_train_cat) \n",
    "cats = ohe.transform(X_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create categorial dataframe with column names\n",
    "columns = ohe.get_feature_names(cat_variable)\n",
    "X_train_cat_df = pd.DataFrame(cats, columns=columns, index=X_train_cat.index)\n",
    "X_train_cat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for test set\n",
    "X_test_cat = X_test[['race', 'gender', 'diag_1', 'diag_2', 'diag_3']]\n",
    "\n",
    "cats_test = ohe.transform(X_test_cat)\n",
    "\n",
    "cat_columns = ohe.get_feature_names(['race', 'gender', 'diag_1', 'diag_2', 'diag_3'])\n",
    "X_test_cat_df = pd.DataFrame(cats_test, columns=cat_columns, index=X_test_cat.index)\n",
    "X_test_cat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for validation set\n",
    "X_val_cat = X_val[['race', 'gender', 'diag_1', 'diag_2', 'diag_3']]\n",
    "\n",
    "cats_val = ohe.transform(X_val_cat)\n",
    "\n",
    "cat_columns = ohe.get_feature_names(['race', 'gender', 'diag_1', 'diag_2', 'diag_3'])\n",
    "X_val_cat_df = pd.DataFrame(cats_val, columns=cat_columns, index=X_val_cat.index)\n",
    "X_val_cat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process continous variables\n",
    "X_train_cont = X_train[['age', 'time_in_hospital', 'num_lab_procedures', 'num_procedures', 'num_medications']]\n",
    "\n",
    "X_train_cont.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize continous variable\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "ss.fit(X_train_cont)\n",
    "X_train_scaled = ss.transform(X_train_cont)\n",
    "\n",
    "cont_columns = X_train_cont.columns\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=cont_columns, index=X_train_cont.index)\n",
    "\n",
    "X_train_scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for test set\n",
    "X_test_cont = X_test[['age', 'time_in_hospital', 'num_lab_procedures', 'num_procedures', 'num_medications']]\n",
    "\n",
    "X_test_scaled = ss.transform(X_test_cont)\n",
    "\n",
    "cont_columns = X_test_cont.columns\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=cont_columns, index=X_test_cont.index)\n",
    "\n",
    "X_test_scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for validation set\n",
    "X_val_cont = X_val[['age', 'time_in_hospital', 'num_lab_procedures', 'num_procedures', 'num_medications']]\n",
    "\n",
    "X_val_scaled = ss.transform(X_val_cont)\n",
    "\n",
    "cont_columns = X_val_cont.columns\n",
    "X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=cont_columns, index=X_val_cont.index)\n",
    "\n",
    "X_val_scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine continous and categorical features for train set\n",
    "X_train_combined = pd.concat([X_train_cat_df, X_train_scaled_df], axis='columns')\n",
    "\n",
    "X_train_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for test set\n",
    "X_test_combined = pd.concat([X_test_cat_df, X_test_scaled_df], axis='columns')\n",
    "\n",
    "X_test_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for validation set\n",
    "X_val_combined = pd.concat([X_val_cat_df, X_val_scaled_df], axis='columns')\n",
    "\n",
    "X_val_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n",
    "Let's now fit our data to a kNN model and look at the accuracy, precision, and recall scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_accuracy(model, x_tr, y_tr, x_te, y_te):\n",
    "    print(\"The accuracy score for {} is...\".format(model))\n",
    "    print(\"Training: {:6.2f}%\".format(100*model.score(x_tr, y_tr)))\n",
    "    print(\"Test set: {:6.2f}%\".format(100*model.score(x_te, y_te)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_and_recall(y_te, y_pred, threshold=0.5):\n",
    "    if threshold != 0.5:\n",
    "        print(\"Threshold of {}...\".format(threshold))\n",
    "    else:\n",
    "        print(\"Default threshold...\")\n",
    "    print(\"Precision: {:6.2f}%, Recall: {:6.2f}%\".format(100*precision_score(y_te, y_pred, zero_division=0),\n",
    "                                                      100*recall_score(y_te, y_pred, zero_division=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores(model, x_tr, y_tr, x_te, y_te, y_pred, threshold=0.5):\n",
    "    '''\n",
    "    This function prints accuracy, precision, and recall scores for a given model\n",
    "    '''\n",
    "    \n",
    "    print(\"The accuracy score for {} is...\".format(model))\n",
    "    print(\"Training: {:6.2f}%\".format(100*model.score(x_tr, y_tr)))\n",
    "    print(\"Test set: {:6.2f}%\".format(100*model.score(x_te, y_te)))\n",
    "    \n",
    "    if threshold != 0.5:\n",
    "        print(\"Threshold of {}...\".format(threshold))\n",
    "    else:\n",
    "        print(\"Default threshold...\")\n",
    "    print(\"Precision: {:6.2f}%, Recall: {:6.2f}%\".format(100*precision_score(y_te, y_pred, zero_division=0),\n",
    "                                                      100*recall_score(y_te, y_pred, zero_division=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_combined, label_train)\n",
    "\n",
    "print_accuracy(knn, X_train_combined, label_train, X_test_combined, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the default threshold of 0.5, which is what vanilla predict does\n",
    "y_predict = knn.predict(X_test_combined)\n",
    "    \n",
    "precision_and_recall(label_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the new threshold of 0.06\n",
    "y_predict = (knn.predict_proba(X_test_combined)[:,1] > 0.06)\n",
    "\n",
    "precision_and_recall(label_test, y_predict, 0.06)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the confusion matrix for our KNeighborsClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print confusion matrix for kNN\n",
    "knn_confusion = confusion_matrix(label_test, knn.predict(X_test_combined))\n",
    "plt.figure(dpi=150)\n",
    "sns.heatmap(knn_confusion, cmap=plt.cm.Blues, annot=True, square=True, fmt='.0f',\n",
    "           xticklabels=df['readmitted'].unique(),\n",
    "           yticklabels=df['readmitted'].unique())\n",
    "\n",
    "plt.xlabel('Predicted species')\n",
    "plt.ylabel('Actual species')\n",
    "plt.title('kNN confusion matrix');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Oversampling\n",
    "Given that there is a class imbalance for the `readmitted` target variable, we may be able to improve the model by utilizing random oversampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['readmitted'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on to a different model, let's try to improve the current models by using random oversampling. We can see that there is a class imbalance in our target variable, so it's always going to be about as accurate as the imbalance itself. In other words, there are 31019 records classifying when a patient is not readmitted to the hospital and just 8706 when a patient is readmitted. That means 72% of the patients in our data aren't readmitted, so a classification model that predicts guesses \"not readmitted\" 72% of the time will be fairly accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some random oversampling of the minority classes\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_resampled, y_resampled = ros.fit_sample(X_train_combined, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yay, balanced classes!\n",
    "Counter(y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis again. What do we expect to see with balanced classes??\n",
    "# cell takes ~15s to run\n",
    "knn_resampled = KNeighborsClassifier()\n",
    "clf_ros = knn_resampled.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Print confusion matrix for kNN regression\n",
    "knn_resampled_confusion = confusion_matrix(label_test, knn_resampled.predict(X_test_combined))\n",
    "plt.figure(dpi=150)\n",
    "sns.heatmap(knn_resampled_confusion, cmap=plt.cm.Blues, annot=True, square=True, fmt='.0f',\n",
    "           xticklabels=[0, 1],\n",
    "           yticklabels=[0, 1])\n",
    "\n",
    "plt.xlabel('Predicted species')\n",
    "plt.ylabel('Actual species')\n",
    "plt.title('Oversampled kNN confusion matrix');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at the scores again for the resampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_accuracy(knn_resampled, X_resampled, y_resampled, X_test_combined, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "precision_and_recall(label_test, knn_resampled.predict(X_test_combined))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the model performance by looking at the predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = knn.predict_proba(X_test_combined)[:,1]\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.hist(prediction[label_test==0], bins=50, label='Negatives', color='b')\n",
    "plt.hist(prediction[label_test==1], bins=50, label='Positives', color='r')\n",
    "plt.xlabel('Probability of being Positive Class', fontsize=25)\n",
    "plt.ylabel('Number of records in each bucket', fontsize=25)\n",
    "plt.legend(fontsize=15)\n",
    "plt.tick_params(axis='both', labelsize=25, pad=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "Let's now look at the same information for the logistic regression model to compare. We'll look at the baseline model and then use random oversampling to see if we can improve, and then we'll look at the scores again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(C = 0.95)\n",
    "logit.fit(X_train_combined, label_train)\n",
    "\n",
    "print_accuracy(logit, X_train_combined, label_train, X_test_combined, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the default threshold of 0.5, which is what vanilla predict does\n",
    "y_predict = logit.predict(X_test_combined)\n",
    "    \n",
    "precision_and_recall(label_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the new threshold of 0.06\n",
    "y_predict = (logit.predict_proba(X_test_combined)[:,1] > 0.06)\n",
    "    \n",
    "precision_and_recall(label_test, y_predict, 0.06)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a prediction using our Linear Regression model. We can see that the logistic regression model predicts an outcome of 0, or not readmitted, roughly 80% of the time and an outcome of 1, or readmitted, about 20% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = logit.predict_proba(X_test_combined)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also look at the confusion matrix for the Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print confusion matrix for logistic regression\n",
    "logit_confusion = confusion_matrix(label_test, logit.predict(X_test_combined))\n",
    "plt.figure(dpi=150)\n",
    "sns.heatmap(logit_confusion, cmap=plt.cm.Blues, annot=True, square=True, fmt='.0f',\n",
    "           xticklabels=[0, 1],\n",
    "           yticklabels=[0, 1])\n",
    "\n",
    "plt.xlabel('Predicted species')\n",
    "plt.ylabel('Actual species')\n",
    "plt.title('Logistic regression confusion matrix');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that our categorical variables have a large number of classes, a logistic model will already be hard to interpret. If we find other models with less interpretability, but with better scores, we should go with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the logistic regression model coefficients\n",
    "logit.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try random oversampling or the LogisticRegression model like we did for our KNeighborsClassifier to see what improvements can be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis again. What do we expect to see with balanced classes??\n",
    "# cell takes ~15s to run\n",
    "lr_resampled = LogisticRegression()\n",
    "clf_ros = lr_resampled.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Print confusion matrix for kNN regression\n",
    "lr_resampled_confusion = confusion_matrix(label_test, lr_resampled.predict(X_test_combined))\n",
    "plt.figure(dpi=150)\n",
    "sns.heatmap(lr_resampled_confusion, cmap=plt.cm.Blues, annot=True, square=True, fmt='.0f',\n",
    "           xticklabels=[0, 1],\n",
    "           yticklabels=[0, 1])\n",
    "\n",
    "plt.xlabel('Predicted species')\n",
    "plt.ylabel('Actual species')\n",
    "plt.title('Oversampled Logistic Regression confusion matrix');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores for the resampled logistic regression model don't seem like an improvement over the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_accuracy(lr_resampled, X_resampled, y_resampled, X_test_combined, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_and_recall(label_test, lr_resampled.predict(X_test_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = logit.predict_proba(X_test_combined)[:,1]\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.hist(prediction[label_test==0], bins=50, label='Negatives', color='b')\n",
    "plt.hist(prediction[label_test==1], bins=50, label='Positives', color='r')\n",
    "plt.xlabel('Probability of being Positive Class', fontsize=25)\n",
    "plt.ylabel('Number of records in each bucket', fontsize=25)\n",
    "plt.legend(fontsize=15)\n",
    "plt.tick_params(axis='both', labelsize=25, pad=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisiontree = DecisionTreeClassifier(max_depth=4)\n",
    "decisiontree.fit(X_train_combined, label_train)\n",
    "y_pred = decisiontree.predict(X_test_combined)\n",
    "\n",
    "print_scores(decisiontree, X_train_combined, label_train, X_test_combined, label_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print confusion matrix for logistic regression\n",
    "dt_confusion = confusion_matrix(label_test, decisiontree.predict(X_test_combined))\n",
    "plt.figure(dpi=150)\n",
    "sns.heatmap(dt_confusion, cmap=plt.cm.Blues, annot=True, square=True, fmt='.0f',\n",
    "           xticklabels=[0, 1],\n",
    "           yticklabels=[0, 1])\n",
    "\n",
    "plt.xlabel('Predicted species')\n",
    "plt.ylabel('Actual species')\n",
    "plt.title('Decision Tree Confusion Matrix');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis again. What do we expect to see with balanced classes??\n",
    "# cell takes ~15s to run\n",
    "dt_resampled = DecisionTreeClassifier()\n",
    "clf_ros = dt_resampled.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Print confusion matrix for kNN regression\n",
    "dt_resampled_confusion = confusion_matrix(label_test, dt_resampled.predict(X_test_combined))\n",
    "plt.figure(dpi=150)\n",
    "sns.heatmap(dt_resampled_confusion, cmap=plt.cm.Blues, annot=True, square=True, fmt='.0f',\n",
    "           xticklabels=[0, 1],\n",
    "           yticklabels=[0, 1])\n",
    "\n",
    "plt.xlabel('Predicted species')\n",
    "plt.ylabel('Actual species')\n",
    "plt.title('Oversampled Decision Tree Confusion Matrix');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_accuracy(dt_resampled, X_resampled, y_resampled, X_test_combined, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_and_recall(label_test, dt_resampled.predict(X_test_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = decisiontree.predict_proba(X_test_combined)[:,1]\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.hist(prediction[label_test==0], bins=50, label='Negatives', color='b')\n",
    "plt.hist(prediction[label_test==1], bins=50, label='Positives', color='r')\n",
    "plt.xlabel('Probability of being Positive Class', fontsize=25)\n",
    "plt.ylabel('Number of records in each bucket', fontsize=25)\n",
    "plt.legend(fontsize=15)\n",
    "plt.tick_params(axis='both', labelsize=25, pad=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "We'll first look at the scores for the model, and then we'll try oversampling and look at the scores again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomforest = RandomForestClassifier(n_estimators=100)\n",
    "randomforest.fit(X_train_combined, label_train)\n",
    "y_pred = randomforest.predict(X_test_combined)\n",
    "\n",
    "print_scores(randomforest, X_train_combined, label_train, X_test_combined, label_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print confusion matrix for logistic regression\n",
    "rf_confusion = confusion_matrix(label_test, randomforest.predict(X_test_combined))\n",
    "plt.figure(dpi=150)\n",
    "sns.heatmap(rf_confusion, cmap=plt.cm.Blues, annot=True, square=True, fmt='.0f',\n",
    "           xticklabels=[0, 1],\n",
    "           yticklabels=[0, 1])\n",
    "\n",
    "plt.xlabel('Predicted species')\n",
    "plt.ylabel('Actual species')\n",
    "plt.title('Decision Tree Confusion Matrix');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis again. What do we expect to see with balanced classes??\n",
    "# cell takes ~15s to run\n",
    "rf_resampled = RandomForestClassifier(n_estimators=100)\n",
    "clf_ros = rf_resampled.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Print confusion matrix for kNN regression\n",
    "rf_resampled_confusion = confusion_matrix(label_test, rf_resampled.predict(X_test_combined))\n",
    "plt.figure(dpi=150)\n",
    "sns.heatmap(rf_resampled_confusion, cmap=plt.cm.Blues, annot=True, square=True, fmt='.0f',\n",
    "           xticklabels=[0, 1],\n",
    "           yticklabels=[0, 1])\n",
    "\n",
    "plt.xlabel('Predicted species')\n",
    "plt.ylabel('Actual species')\n",
    "plt.title('Oversampled Random Forest confusion matrix');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_accuracy(rf_resampled, X_resampled, y_resampled, X_test_combined, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_and_recall(label_test, rf_resampled.predict(X_test_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = randomforest.predict_proba(X_test_combined)[:,1]\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.hist(prediction[label_test==0], bins=50, label='Negatives', color='b')\n",
    "plt.hist(prediction[label_test==1], bins=50, label='Positives', color='r')\n",
    "plt.xlabel('Probability of being Positive Class', fontsize=15)\n",
    "plt.ylabel('Number of records in each bucket', fontsize=15)\n",
    "plt.title('Performance Analysis of Random Forest Model', fontsize=25)\n",
    "plt.legend(fontsize=15)\n",
    "plt.tick_params(axis='both', labelsize=25, pad=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = rf_resampled.predict_proba(X_test_combined)[:,1]\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.hist(prediction[label_test==0], bins=50, label='Negatives', color='b')\n",
    "plt.hist(prediction[label_test==1], bins=50, label='Positives', color='r')\n",
    "plt.xlabel('Probability of being Positive Class', fontsize=15)\n",
    "plt.ylabel('Number of records in each bucket', fontsize=15)\n",
    "plt.title('Performance Analysis of Random Forest Model', fontsize=25)\n",
    "plt.legend(fontsize=15)\n",
    "plt.tick_params(axis='both', labelsize=25, pad=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(importance,names,model_type):\n",
    "\n",
    "    #Create arrays from feature importance and feature names\n",
    "    feature_importance = np.array(importance)\n",
    "    feature_names = np.array(names)\n",
    "\n",
    "    #Create a DataFrame using a Dictionary\n",
    "    data={'feature_names':feature_names,'feature_importance':feature_importance}\n",
    "    fi_df = pd.DataFrame(data)\n",
    "\n",
    "    #Sort the DataFrame in order decreasing feature importance\n",
    "    fi_df.sort_values(by=['feature_importance'], ascending=False, inplace=True)\n",
    "    fi_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Filter out the top 20 indices in feature_importance column\n",
    "    bottom_indices = fi_df[(fi_df['feature_importance'] < 0.01)].index\n",
    "\n",
    "    # Delete these row indexes from DataFrame\n",
    "    fi_df.drop(bottom_indices, inplace=True)\n",
    "\n",
    "    #Define size of bar plot\n",
    "    plt.figure(figsize=(10,8))\n",
    "    #Plot Searborn bar chart\n",
    "    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n",
    "    #Add chart labels\n",
    "    plt.title(model_type + 'Feature Importance', fontsize=25)\n",
    "    plt.xlabel('Feature Importance', fontsize=15)\n",
    "    plt.ylabel('Feature Name', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(rf_resampled.feature_importances_, X_resampled.columns, 'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = xgb.XGBClassifier( \n",
    "        n_estimators=30000,\n",
    "        max_depth=4,\n",
    "        objective='binary:logistic', #new objective\n",
    "        use_label_encoder=False,\n",
    "        learning_rate=.05, \n",
    "        subsample=.8,\n",
    "        min_child_weight=3,\n",
    "        colsample_bytree=.8)\n",
    "\n",
    "eval_set=[(X_train_combined, label_train), (X_val_combined, label_val)]\n",
    "\n",
    "fit_model = gbm.fit(X_train_combined, label_train,\n",
    "                    eval_set=eval_set, eval_metric='error',\n",
    "                    early_stopping_rounds=50, verbose=False)\n",
    "\n",
    "y_pred = gbm.predict(X_test_combined)\n",
    "\n",
    "print_scores(gbm, X_train_combined, label_train, X_test_combined, label_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print confusion matrix for logistic regression\n",
    "rf_confusion = confusion_matrix(label_test, randomforest.predict(X_test_combined))\n",
    "plt.figure(dpi=150)\n",
    "sns.heatmap(rf_confusion, cmap=plt.cm.Blues, annot=True, square=True, fmt='.0f',\n",
    "           xticklabels=[0, 1],\n",
    "           yticklabels=[0, 1])\n",
    "\n",
    "plt.xlabel('Predicted species')\n",
    "plt.ylabel('Actual species')\n",
    "plt.title('Decision Tree Confusion Matrix');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis again. What do we expect to see with balanced classes??\n",
    "# cell takes ~15s to run\n",
    "rf_resampled = RandomForestClassifier(n_estimators=100)\n",
    "clf_ros = rf_resampled.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Print confusion matrix for kNN regression\n",
    "rf_resampled_confusion = confusion_matrix(label_test, rf_resampled.predict(X_test_combined))\n",
    "plt.figure(dpi=150)\n",
    "sns.heatmap(rf_resampled_confusion, cmap=plt.cm.Blues, annot=True, square=True, fmt='.0f',\n",
    "           xticklabels=[0, 1],\n",
    "           yticklabels=[0, 1])\n",
    "\n",
    "plt.xlabel('Predicted species')\n",
    "plt.ylabel('Actual species')\n",
    "plt.title('Oversampled Random Forest confusion matrix');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = knn.predict_proba(X_test_combined)[:,1]\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.hist(prediction[label_test==0], bins=50, label='Negatives', color='b')\n",
    "plt.hist(prediction[label_test==1], bins=50, label='Positives', color='r')\n",
    "plt.xlabel('Probability of being Positive Class', fontsize=25)\n",
    "plt.ylabel('Number of records in each bucket', fontsize=25)\n",
    "plt.legend(fontsize=15)\n",
    "plt.tick_params(axis='both', labelsize=25, pad=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Models\n",
    "Evaluate the RMSE of various models used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(actuals, preds):\n",
    "    '''\n",
    "    Function to calculate Root Mean Squared Error\n",
    "    '''\n",
    "    return np.sqrt(((actuals - preds) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['kNN', 'LogisticRegression', 'DecisionTree', 'RandomForest', 'XGBoost']\n",
    "for key in models.items():\n",
    "    print(\"RMSE for {}: {}\\n\".format(key, rmse(key.predict(X_test_combined), label_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "# Add the models to the list that you want to view on the ROC plot\n",
    "models = [\n",
    "{\n",
    "    'label': 'k Nearest Neighbors',\n",
    "    'model': KNeighborsClassifier(n_neighbors=5),\n",
    "},\n",
    "{\n",
    "    'label': 'Logistic Regression',\n",
    "    'model': LogisticRegression(C=0.95),\n",
    "},\n",
    "{\n",
    "    'label': 'Decision Tree',\n",
    "    'model': DecisionTreeClassifier(max_depth=4),\n",
    "},\n",
    "{\n",
    "    'label': 'Random Forest',\n",
    "    'model': RandomForestClassifier(n_estimators=100),\n",
    "}\n",
    "]\n",
    "\n",
    "# Below for loop iterates through your models list\n",
    "for m in models:\n",
    "    model = m['model'] # select the model\n",
    "    model.fit(X_train_combined, label_train) # train the model\n",
    "    y_pred = model.predict(X_test_combined) # predict the test data\n",
    "    # Compute False postive rate, and True positive rate\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(label_test, model.predict_proba(X_test_combined)[:,1])\n",
    "    # Calculate Area under the curve to display on the plot\n",
    "    auc = metrics.roc_auc_score(label_test, model.predict(X_test_combined))\n",
    "    # Now, plot the computed values\n",
    "    plt.plot(fpr, tpr, label='%s ROC (area = %0.2f)' % (m['label'], auc))\n",
    "# Custom settings for the plot \n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1-Specificity(False Positive Rate)')\n",
    "plt.ylabel('Sensitivity(True Positive Rate)')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()   # Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "# Add the models to the list that you want to view on the ROC plot\n",
    "models = [\n",
    "{\n",
    "    'label': 'k Nearest Neighbors',\n",
    "    'model': KNeighborsClassifier(n_neighbors=5),\n",
    "},\n",
    "{\n",
    "    'label': 'Logistic Regression',\n",
    "    'model': LogisticRegression(C=0.95),\n",
    "},\n",
    "{\n",
    "    'label': 'Decision Tree',\n",
    "    'model': DecisionTreeClassifier(max_depth=4),\n",
    "},\n",
    "{\n",
    "    'label': 'Random Forest',\n",
    "    'model': RandomForestClassifier(n_estimators=100),\n",
    "}\n",
    "]\n",
    "\n",
    "# Below for loop iterates through your models list\n",
    "for m in models:\n",
    "    model = m['model'] # select the model\n",
    "    model.fit(X_resampled, y_resampled) # train the model\n",
    "    y_pred = model.predict(X_test_combined) # predict the test data\n",
    "    # Compute False postive rate, and True positive rate\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(label_test, model.predict_proba(X_test_combined)[:,1])\n",
    "    # Calculate Area under the curve to display on the plot\n",
    "    auc = metrics.roc_auc_score(label_test, model.predict(X_test_combined))\n",
    "    # Now, plot the computed values\n",
    "    plt.plot(fpr, tpr, label='%s ROC (area = %0.2f)' % (m['label'], auc))\n",
    "# Custom settings for the plot \n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1-Specificity(False Positive Rate)', fontsize=15)\n",
    "plt.ylabel('Sensitivity(True Positive Rate)', fontsize=15)\n",
    "plt.title('Receiver Operating Characteristic', fontsize=25)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()   # Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
